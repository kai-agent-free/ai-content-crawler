{
	"url": "https://docs.apify.com/academy/scraping-basics-javascript?legacy-js-course=/",
	"title": "Web scraping basics for JavaScript devs | Academy | Apify Documentation",
	"content": "",
	"metadata": {
		"description": "Learn how to use JavaScript to extract information from websites in this practical course, starting from the absolute basics.",
		"language": "en",
		"wordCount": 925,
		"crawledAt": "2026-02-14T05:28:52.870Z"
	},
	"markdown": "**Learn how to use JavaScript to extract information from websites in this practical course, starting from the absolute basics.**\n\n* * *\n\nIn this course we'll use JavaScript to create an application for watching prices. It'll be able to scrape all product pages of an e-commerce website and record prices. Data from several runs of such program would be useful for seeing trends in price changes, detecting discounts, etc.\n\n![E-commerce listing on the left, JSON with data on the right](/assets/images/scraping-7add65f10b823af8c68c3f30a39dd679.webp)\n\n## What we'll do[](#what-well-do)\n\n-   Inspect pages using browser DevTools.\n-   Download web pages using the Fetch API.\n-   Extract data from web pages using the Cheerio library.\n-   Save extracted data in various formats (e.g. CSV which MS Excel or Google Sheets can open) using the json2csv library.\n-   Follow links programmatically (crawling).\n-   Save time and effort with frameworks, such as Crawlee, and scraping platforms, such as Apify.\n\n## Who this course is for[](#who-this-course-is-for)\n\nAnyone with basic knowledge of developing programs in JavaScript who wants to start with web scraping can take this course. The course does not expect you to have any prior knowledge of other web technologies or scraping.\n\n## Requirements[](#requirements)\n\n-   A macOS, Linux, or Windows machine with a web browser and Node.js installed.\n-   Familiarity with JavaScript basics: variables, conditions, loops, functions, strings, arrays, objects, files, classes, promises, imports, and exceptions.\n-   Comfort with building a Node.js package and installing dependencies with `npm`.\n-   Familiarity with running commands in Terminal (macOS/Linux) or Command Prompt (Windows).\n\n## You may want to know[](#you-may-want-to-know)\n\nLet's explore the key reasons to take this course. What is web scraping good for, and what career opportunities does it enable for you?\n\n### Why learn scraping[](#why-learn-scraping)\n\nThe internet is full of useful data, but most of it isn't offered in a structured way that's easy to process programmatically. That's why you need scraping, a set of approaches to download websites and extract data from them.\n\nScraper development is also a fun and challenging way to learn web development, web technologies, and understand the internet. You'll reverse-engineer websites, understand how they work internally, discover what technologies they use, and learn how they communicate with servers. You'll also master your chosen programming language and core programming concepts. Understanding web scraping gives you a head start in learning web technologies such as HTML, CSS, JavaScript, frontend frameworks (like React or Next.js), HTTP, REST APIs, GraphQL APIs, and more.\n\n### Why build your own scrapers[](#why-build-your-own-scrapers)\n\nScrapers are programs specifically designed to mine data from the internet. Point-and-click or no-code scraping solutions do exist, but they only take you so far. While simple to use, they lack the flexibility and optimization needed to handle advanced cases. Only custom-built scrapers can tackle more difficult challenges. And unlike ready-made solutions, they can be fine-tuned to perform tasks more efficiently, at a lower cost, or with greater precision.\n\n### Why become a scraper dev[](#why-become-a-scraper-dev)\n\nAs a scraper developer, you are not limited by whether certain data is available programmatically through an official API‚Äîthe entire web becomes your API! Here are some things you can do if you understand scraping:\n\n-   Improve your productivity by building personal tools, such as your own real estate or rare sneakers watchdog.\n-   Companies can hire you to build custom scrapers mining data important for their business.\n-   Become an invaluable asset to data journalism, data science, or nonprofit teams working to make the world a better place.\n-   You can publish your scrapers on platforms like the [Apify Store](https://apify.com/store) and earn money by renting them out to others.\n\n### Why learn with Apify[](#why-learn-with-apify)\n\nWe are [Apify](https://apify.com), a web scraping and automation platform. We do our best to build this course on top of open source technologies. That means what you learn applies to any scraping project, and you'll be able to run your scrapers on any computer. We will show you how a scraping platform can simplify your life, but that lesson is optional and designed to fit within our [free tier](https://apify.com/pricing).\n\n## Course content[](#course-content)\n\n[\n\n## üìÑÔ∏è DevTools: Inspecting\n\nLesson about using the browser tools for developers to inspect and manipulate the structure of a website.\n\n](/academy/scraping-basics-javascript/devtools-inspecting)\n\n[\n\n## üìÑÔ∏è DevTools: Locating HTML elements\n\nLesson about using the browser tools for developers to manually find products on an e-commerce website.\n\n](/academy/scraping-basics-javascript/devtools-locating-elements)\n\n[\n\n## üìÑÔ∏è DevTools: Extracting data\n\nLesson about using the browser tools for developers to manually extract product data from an e-commerce website.\n\n](/academy/scraping-basics-javascript/devtools-extracting-data)\n\n[\n\n## üìÑÔ∏è Downloading HTML\n\nLesson about building a Node.js application for watching prices. Using the Fetch API to download HTML code of a product listing page.\n\n](/academy/scraping-basics-javascript/downloading-html)\n\n[\n\n## üìÑÔ∏è Parsing HTML\n\nLesson about building a Node.js application for watching prices. Using the Cheerio library to parse HTML code of a product listing page.\n\n](/academy/scraping-basics-javascript/parsing-html)\n\n[\n\n## üìÑÔ∏è Locating HTML elements\n\nLesson about building a Node.js application for watching prices. Using the Cheerio library to locate products on the product listing page.\n\n](/academy/scraping-basics-javascript/locating-elements)\n\n[\n\n## üìÑÔ∏è Extracting data from HTML\n\nLesson about building a Node.js application for watching prices. Using string manipulation to extract and clean data scraped from the product listing page.\n\n](/academy/scraping-basics-javascript/extracting-data)\n\n[\n\n## üìÑÔ∏è Saving data\n\nLesson about building a Node.js application for watching prices. Using the json2csv library to save data scraped from product listing pages in both JSON and CSV.\n\n](/academy/scraping-basics-javascript/saving-data)\n\n[\n\n## üìÑÔ∏è Getting links from HTML\n\nLesson about building a Node.js application for watching prices. Using the Cheerio library to locate links to individual product pages.\n\n](/academy/scraping-basics-javascript/getting-links)\n\n[\n\n## üìÑÔ∏è Crawling websites\n\nLesson about building a Node.js application for watching prices. Using the Fetch API to follow links to individual product pages.\n\n](/academy/scraping-basics-javascript/crawling)\n\n[\n\n## üìÑÔ∏è Scraping product variants\n\nLesson about building a Node.js application for watching prices. Using browser DevTools to figure out how to extract product variants and exporting them as separate items.\n\n](/academy/scraping-basics-javascript/scraping-variants)\n\n[\n\n## üìÑÔ∏è Using a framework\n\nLesson about building a Node.js application for watching prices. Using the Crawlee framework to simplify creating a scraper.\n\n](/academy/scraping-basics-javascript/framework)\n\n[\n\n## üìÑÔ∏è Using a platform\n\nLesson about building a Node.js application for watching prices. Using the Apify platform to deploy a scraper.\n\n](/academy/scraping-basics-javascript/platform)",
	"chunks": [
		{
			"text": "**Learn how to use JavaScript to extract information from websites in this practical course, starting from the absolute basics.**\n\n* * *\n\nIn this course we'll use JavaScript to create an application for watching prices. It'll be able to scrape all product pages of an e-commerce website and record prices. Data from several runs of such program would be useful for seeing trends in price changes, detecting discounts, etc. ![E-commerce listing on the left, JSON with data on the right](/assets/images/scraping-7add65f10b823af8c68c3f30a39dd679.webp)\n\n## What we'll do[](#what-well-do)\n\n-   Inspect pages using browser DevTools. -   Download web pages using the Fetch API. -   Extract data from web pages using the Cheerio library. -   Save extracted data in various formats (e.g. CSV which MS Excel or Google Sheets can open) using the json2csv library. -   Follow links programmatically (crawling). -   Save time and effort with frameworks, such as Crawlee, and scraping platforms, such as Apify.",
			"index": 0,
			"metadata": {
				"charCount": 996
			}
		},
		{
			"text": "extracted data in various formats (e.g. CSV which MS Excel or Google Sheets can open) using the json2csv library. - Follow links programmatically (crawling). - Save time and effort with frameworks, such as Crawlee, and scraping platforms, such as Apify. ## Who this course is for[](#who-this-course-is-for)\n\nAnyone with basic knowledge of developing programs in JavaScript who wants to start with web scraping can take this course. The course does not expect you to have any prior knowledge of other web technologies or scraping. ## Requirements[](#requirements)\n\n-   A macOS, Linux, or Windows machine with a web browser and Node.js installed. -   Familiarity with JavaScript basics: variables, conditions, loops, functions, strings, arrays, objects, files, classes, promises, imports, and exceptions. -   Comfort with building a Node.js package and installing dependencies with `npm`. -   Familiarity with running commands in Terminal (macOS/Linux) or Command Prompt (Windows).",
			"index": 1,
			"metadata": {
				"charCount": 979
			}
		},
		{
			"text": "with JavaScript basics: variables, conditions, loops, functions, strings, arrays, objects, files, classes, promises, imports, and exceptions. - Comfort with building a Node.js package and installing dependencies with `npm`. - Familiarity with running commands in Terminal (macOS/Linux) or Command Prompt (Windows). ## You may want to know[](#you-may-want-to-know)\n\nLet's explore the key reasons to take this course. What is web scraping good for, and what career opportunities does it enable for you? ### Why learn scraping[](#why-learn-scraping)\n\nThe internet is full of useful data, but most of it isn't offered in a structured way that's easy to process programmatically. That's why you need scraping, a set of approaches to download websites and extract data from them. Scraper development is also a fun and challenging way to learn web development, web technologies, and understand the internet.",
			"index": 2,
			"metadata": {
				"charCount": 900
			}
		},
		{
			"text": "easy to process programmatically. That's why you need scraping, a set of approaches to download websites and extract data from them. Scraper development is also a fun and challenging way to learn web development, web technologies, and understand the internet. You'll reverse-engineer websites, understand how they work internally, discover what technologies they use, and learn how they communicate with servers. You'll also master your chosen programming language and core programming concepts. Understanding web scraping gives you a head start in learning web technologies such as HTML, CSS, JavaScript, frontend frameworks (like React or Next.js), HTTP, REST APIs, GraphQL APIs, and more. ### Why build your own scrapers[](#why-build-your-own-scrapers)\n\nScrapers are programs specifically designed to mine data from the internet. Point-and-click or no-code scraping solutions do exist, but they only take you so far.",
			"index": 3,
			"metadata": {
				"charCount": 919
			}
		},
		{
			"text": "or Next.js), HTTP, REST APIs, GraphQL APIs, and more. ### Why build your own scrapers[](#why-build-your-own-scrapers) Scrapers are programs specifically designed to mine data from the internet. Point-and-click or no-code scraping solutions do exist, but they only take you so far. While simple to use, they lack the flexibility and optimization needed to handle advanced cases. Only custom-built scrapers can tackle more difficult challenges. And unlike ready-made solutions, they can be fine-tuned to perform tasks more efficiently, at a lower cost, or with greater precision. ### Why become a scraper dev[](#why-become-a-scraper-dev)\n\nAs a scraper developer, you are not limited by whether certain data is available programmatically through an official API‚Äîthe entire web becomes your API! Here are some things you can do if you understand scraping:\n\n-   Improve your productivity by building personal tools, such as your own real estate or rare sneakers watchdog.",
			"index": 4,
			"metadata": {
				"charCount": 966
			}
		},
		{
			"text": "available programmatically through an official API‚Äîthe entire web becomes your API! Here are some things you can do if you understand scraping: - Improve your productivity by building personal tools, such as your own real estate or rare sneakers watchdog. -   Companies can hire you to build custom scrapers mining data important for their business. -   Become an invaluable asset to data journalism, data science, or nonprofit teams working to make the world a better place. -   You can publish your scrapers on platforms like the [Apify Store](https://apify.com/store) and earn money by renting them out to others. ### Why learn with Apify[](#why-learn-with-apify)\n\nWe are [Apify](https://apify.com), a web scraping and automation platform. We do our best to build this course on top of open source technologies. That means what you learn applies to any scraping project, and you'll be able to run your scrapers on any computer.",
			"index": 5,
			"metadata": {
				"charCount": 930
			}
		},
		{
			"text": "web scraping and automation platform. We do our best to build this course on top of open source technologies. That means what you learn applies to any scraping project, and you'll be able to run your scrapers on any computer. We will show you how a scraping platform can simplify your life, but that lesson is optional and designed to fit within our [free tier](https://apify.com/pricing). ## Course content[](#course-content)\n\n[\n\n## üìÑÔ∏è DevTools: Inspecting\n\nLesson about using the browser tools for developers to inspect and manipulate the structure of a website. ](/academy/scraping-basics-javascript/devtools-inspecting)\n\n[\n\n## üìÑÔ∏è DevTools: Locating HTML elements\n\nLesson about using the browser tools for developers to manually find products on an e-commerce website. ](/academy/scraping-basics-javascript/devtools-locating-elements)\n\n[\n\n## üìÑÔ∏è DevTools: Extracting data\n\nLesson about using the browser tools for developers to manually extract product data from an e-commerce website.",
			"index": 6,
			"metadata": {
				"charCount": 990
			}
		},
		{
			"text": "Lesson about using the browser tools for developers to manually find products on an e-commerce website. ](/academy/scraping-basics-javascript/devtools-locating-elements) [ ## üìÑÔ∏è DevTools: Extracting data Lesson about using the browser tools for developers to manually extract product data from an e-commerce website. ](/academy/scraping-basics-javascript/devtools-extracting-data)\n\n[\n\n## üìÑÔ∏è Downloading HTML\n\nLesson about building a Node.js application for watching prices. Using the Fetch API to download HTML code of a product listing page. ](/academy/scraping-basics-javascript/downloading-html)\n\n[\n\n## üìÑÔ∏è Parsing HTML\n\nLesson about building a Node.js application for watching prices. Using the Cheerio library to parse HTML code of a product listing page. ](/academy/scraping-basics-javascript/parsing-html)\n\n[\n\n## üìÑÔ∏è Locating HTML elements\n\nLesson about building a Node.js application for watching prices. Using the Cheerio library to locate products on the product listing page.",
			"index": 7,
			"metadata": {
				"charCount": 988
			}
		},
		{
			"text": "the Cheerio library to parse HTML code of a product listing page. ](/academy/scraping-basics-javascript/parsing-html) [ ## üìÑÔ∏è Locating HTML elements Lesson about building a Node.js application for watching prices. Using the Cheerio library to locate products on the product listing page. ](/academy/scraping-basics-javascript/locating-elements)\n\n[\n\n## üìÑÔ∏è Extracting data from HTML\n\nLesson about building a Node.js application for watching prices. Using string manipulation to extract and clean data scraped from the product listing page. ](/academy/scraping-basics-javascript/extracting-data)\n\n[\n\n## üìÑÔ∏è Saving data\n\nLesson about building a Node.js application for watching prices. Using the json2csv library to save data scraped from product listing pages in both JSON and CSV. ](/academy/scraping-basics-javascript/saving-data)\n\n[\n\n## üìÑÔ∏è Getting links from HTML\n\nLesson about building a Node.js application for watching prices.",
			"index": 8,
			"metadata": {
				"charCount": 932
			}
		},
		{
			"text": "a Node.js application for watching prices. Using the json2csv library to save data scraped from product listing pages in both JSON and CSV. ](/academy/scraping-basics-javascript/saving-data) [ ## üìÑÔ∏è Getting links from HTML Lesson about building a Node.js application for watching prices. Using the Cheerio library to locate links to individual product pages. ](/academy/scraping-basics-javascript/getting-links)\n\n[\n\n## üìÑÔ∏è Crawling websites\n\nLesson about building a Node.js application for watching prices. Using the Fetch API to follow links to individual product pages. ](/academy/scraping-basics-javascript/crawling)\n\n[\n\n## üìÑÔ∏è Scraping product variants\n\nLesson about building a Node.js application for watching prices. Using browser DevTools to figure out how to extract product variants and exporting them as separate items. ](/academy/scraping-basics-javascript/scraping-variants)\n\n[\n\n## üìÑÔ∏è Using a framework\n\nLesson about building a Node.js application for watching prices.",
			"index": 9,
			"metadata": {
				"charCount": 982
			}
		},
		{
			"text": "building a Node.js application for watching prices. Using browser DevTools to figure out how to extract product variants and exporting them as separate items. ](/academy/scraping-basics-javascript/scraping-variants) [ ## üìÑÔ∏è Using a framework Lesson about building a Node.js application for watching prices. Using the Crawlee framework to simplify creating a scraper. ](/academy/scraping-basics-javascript/framework)\n\n[\n\n## üìÑÔ∏è Using a platform\n\nLesson about building a Node.js application for watching prices. Using the Apify platform to deploy a scraper. ](/academy/scraping-basics-javascript/platform)",
			"index": 10,
			"metadata": {
				"charCount": 604
			}
		}
	]
}